{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # CS551 Final Exam: Generative Adversarial Neural Networks\n",
    "\n",
    "\n",
    "\n",
    " **Authors:** \n",
    "### Final Exam\n",
    "\n",
    "Fabio Cozzuto\tStudent id: 002214965\n",
    "\n",
    "Johan Mogollon\tStudent id: 002359844\n",
    "\n",
    " **Contributions:**\n",
    "\n",
    " - **Fabio Cozzuto:** All code, experiments, and analysis\n",
    "\n",
    " - **Johan Mogollon:** All code, experiments, and analysis\n",
    "\n",
    "\n",
    "\n",
    " This notebook demonstrates:\n",
    "\n",
    " 1. Padding calculation for DCGAN discriminator\n",
    "\n",
    " 2. Data‚Äëaugmentation pipelines (`basic` vs. `deluxe`)\n",
    "\n",
    " 3. Visualizing DCGAN samples\n",
    "\n",
    " 4. Plotting DCGAN training losses\n",
    "\n",
    " 5. Comparing CycleGAN outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 1: Deep Convolutional GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Environment Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Ensure Jupyter can import our GAN modules\n",
    "import os, sys\n",
    "sys.path.insert(0, os.path.abspath('.'))\n",
    "sys.path.append('.') \n",
    "\n",
    "# --- Standard Libraries ---\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    " \n",
    "\n",
    "# --- Data Handling ---\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    " \n",
    "\n",
    "# --- PyTorch ---\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# --- Local Modules ---\n",
    "from data_loader import get_data_loader, CustomDataSet\n",
    "from models import DCGenerator, DCDiscriminator, CycleGenerator, conv, deconv, ResnetBlock\n",
    "from utils import to_var, to_data, create_dir\n",
    "\n",
    "\n",
    "# --- Visualization ---\n",
    "import matplotlib.pyplot as plt\n",
    "import imageio # For saving images\n",
    "\n",
    "\n",
    "# --- Argument Parsing ---\n",
    "import argparse\n",
    " \n",
    "\n",
    "# --- Other ---\n",
    "import glob\n",
    " \n",
    "# Set random seed\n",
    "SEED = 11\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "  torch.cuda.manual_seed(SEED)\n",
    " \n",
    "\n",
    "# Check for GPU availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Data Augmentation [10 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We did the implementation of the augmentations in the code:\n",
    "![alt text](augmentations.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement the Discriminator of the DCGAN [10 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 1. Padding Calculation for DCGAN Discriminator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Question:** With kernel size \\(K=4\\) and stride \\(S=2\\), what padding \\(P\\) halves the spatial dimensions?\n",
    "\n",
    "\n",
    "\n",
    " **Answer:** We want each layer to reduce the spatial dimensions by a factor of 2, without clipping important features. That means we want to control the padding. So, we have the convolution output formula:\n",
    "\n",
    "```math\n",
    "O = \\left \\lfloor \\frac{I + 2P - K}{S} \\right \\rfloor + 1\n",
    "```\n",
    "Where:\n",
    "- \\( I \\) = input size\n",
    "- \\( O \\) = output size\n",
    "- \\( K = 4 \\) (kernel size)\n",
    "- \\( S = 2 \\) (stride)\n",
    "- \\( P \\) = padding\n",
    "\n",
    "We want to obtain this:\n",
    "```math\n",
    "\\text output\\_size = \\frac{input\\_size}{2}\n",
    "```\n",
    "So we solve with given: \n",
    "\n",
    "```math\n",
    "\\left\\lfloor \\frac{I + 2P - 4}{2} \\right\\rfloor + 1 = \\frac{I}{2}\n",
    "\\Rightarrow 2P = 2 \\Rightarrow P = 1\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given kernel_size=4, stride=2, the required padding is: 1\n",
      "Example: Input size = 64, Output size = 32.0\n"
     ]
    }
   ],
   "source": [
    "# We can do the same calculation with the following code:\n",
    "\n",
    "input_size = 64  # Example input size, this will vary per layer\n",
    "kernel_size = 4\n",
    "stride = 2\n",
    "padding = 1\n",
    "output_size = (input_size - kernel_size + 2 * padding) / stride + 1\n",
    "\n",
    "\n",
    "print(f\"Given kernel_size={kernel_size}, stride={stride}, the required padding is: {padding}\")\n",
    "print(f\"Example: Input size = {input_size}, Output size = {output_size}\")\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. DCDiscriminator class in the models.py file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](DCDiscriminator.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement the Generator of the DCGAN [10 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. DCGenerator class in the models.py file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](DCGenerator.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Experiments\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Implement the DCGAN Training Loop [10 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discriminator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](DCGANDisc.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](DCGANGen.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Train the DCGAN [10 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next code traiin the DCGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "def train_dcgan(data_aug_mode, num_epochs=100):\n",
    "    \"\"\"\n",
    "    Trains the DCGAN using the vanilla_gan.py script.\n",
    "    Args:\n",
    "    data_aug_mode (str): The data augmentation mode ('basic' or 'deluxe').\n",
    "    num_epochs (int, optional): The number of epochs to train for. Defaults to 100.\n",
    "    \"\"\"\n",
    "    # Construct the command\n",
    "    command = [\n",
    "    \"python\",\n",
    "    \"vanilla_gan.py\",\n",
    "    f\"--data_aug={data_aug_mode}\",\n",
    "    f\"--num_epochs={num_epochs}\"\n",
    "    ]\n",
    "\n",
    "    # Execute the command\n",
    "    process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    stdout, stderr = process.communicate()\n",
    "\n",
    "\n",
    "    # Print the output (optional, but helpful for debugging)\n",
    "    print(stdout.decode())\n",
    "    if stderr:\n",
    "        print(stderr.decode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(image_size=64, conv_dim=32, noise_size=100, num_epochs=100, batch_size=16, num_workers=0, lr=0.0003, beta1=0.5, beta2=0.999, data='cat/grumpifyBprocessed', data_aug='basic', ext='*.png', checkpoint_dir='./checkpoints_vanilla', sample_dir='output/./vanilla\\\\grumpifyBprocessed_basic', log_step=10, sample_every=200, checkpoint_every=400)\n",
      "data/cat/grumpifyBprocessed\\*.png\n",
      "204\n",
      "                    G                  \n",
      "---------------------------------------\n",
      "DCGenerator(\n",
      "  (deconv1): Sequential(\n",
      "    (0): ConvTranspose2d(100, 256, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (deconv2): Sequential(\n",
      "    (0): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (deconv3): Sequential(\n",
      "    (0): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (deconv4): Sequential(\n",
      "    (0): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (deconv5): Sequential(\n",
      "    (0): ConvTranspose2d(32, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  )\n",
      ")\n",
      "---------------------------------------\n",
      "                    D                  \n",
      "---------------------------------------\n",
      "DCDiscriminator(\n",
      "  (conv1): Sequential(\n",
      "    (0): Conv2d(3, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  )\n",
      "  (conv2): Sequential(\n",
      "    (0): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (conv3): Sequential(\n",
      "    (0): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (conv4): Sequential(\n",
      "    (0): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (conv5): Conv2d(256, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      ")\n",
      "---------------------------------------\n",
      "Models moved to GPU.\n",
      "\n",
      "2025-04-17 20:01:34.406558: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-04-17 20:01:35.257519: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "'rm' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\cuda\\Loss.cu:106: block: [0,0,0], thread: [0,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\cuda\\Loss.cu:106: block: [0,0,0], thread: [1,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\cuda\\Loss.cu:106: block: [0,0,0], thread: [2,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\cuda\\Loss.cu:106: block: [0,0,0], thread: [3,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\cuda\\Loss.cu:106: block: [0,0,0], thread: [4,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\cuda\\Loss.cu:106: block: [0,0,0], thread: [5,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\cuda\\Loss.cu:106: block: [0,0,0], thread: [6,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\cuda\\Loss.cu:106: block: [0,0,0], thread: [7,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\cuda\\Loss.cu:106: block: [0,0,0], thread: [8,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\cuda\\Loss.cu:106: block: [0,0,0], thread: [9,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\cuda\\Loss.cu:106: block: [0,0,0], thread: [10,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\cuda\\Loss.cu:106: block: [0,0,0], thread: [11,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\cuda\\Loss.cu:106: block: [0,0,0], thread: [12,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\cuda\\Loss.cu:106: block: [0,0,0], thread: [13,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\cuda\\Loss.cu:106: block: [0,0,0], thread: [14,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\cuda\\Loss.cu:106: block: [0,0,0], thread: [15,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\johan\\Downloads\\GitHub\\DeepLearningDCGAN\\vanilla_gan.py\", line 288, in <module>\n",
      "    main(opts)\n",
      "  File \"c:\\Users\\johan\\Downloads\\GitHub\\DeepLearningDCGAN\\vanilla_gan.py\", line 239, in main\n",
      "    training_loop(dataloader, opts)\n",
      "  File \"c:\\Users\\johan\\Downloads\\GitHub\\DeepLearningDCGAN\\vanilla_gan.py\", line 173, in training_loop\n",
      "    D_real_loss = criterion(D(real_images), real_labels)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\johan\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\johan\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\johan\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\loss.py\", line 697, in forward\n",
      "    return F.binary_cross_entropy(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\johan\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\functional.py\", line 3554, in binary_cross_entropy\n",
      "    return torch._C._nn.binary_cross_entropy(input, target, weight, reduction_enum)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "RuntimeError: CUDA error: device-side assert triggered\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Basic usage\n",
    "train_dcgan('basic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deluxe augmentation\n",
    "train_dcgan('deluxe') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 3. Data‚ÄëAugmentation Pipelines\n",
    "\n",
    "\n",
    "\n",
    " We define both **basic** and **deluxe** transforms.\n",
    "\n",
    " Deluxe uses a 10% up‚Äëscale + random crop + flip :contentReference[oaicite:3]{index=3}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_loader import get_data_loader  # re-import for clarity\n",
    "\n",
    "# Suppose opts.image_size is 64 for this exam\n",
    "image_size = 64\n",
    "\n",
    "basic_transform = transforms.Compose([\n",
    "    transforms.Resize(image_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,)*3, (0.5,)*3),\n",
    "])\n",
    "\n",
    "deluxe_transform = transforms.Compose([\n",
    "    transforms.Resize(int(1.1 * image_size)),\n",
    "    transforms.RandomCrop(image_size),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,)*3, (0.5,)*3),\n",
    "])\n",
    "\n",
    "# Display example\n",
    "sample_img = Image.open('data/sample.png').convert('RGB')\n",
    "fig, axes = plt.subplots(1, 2, figsize=(8,4))\n",
    "axes[0].imshow(np.transpose(basic_transform(sample_img).numpy(), (1,2,0)))\n",
    "axes[0].set_title(\"Basic\")\n",
    "axes[0].axis('off')\n",
    "axes[1].imshow(np.transpose(deluxe_transform(sample_img).numpy(), (1,2,0)))\n",
    "axes[1].set_title(\"Deluxe\")\n",
    "axes[1].axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 4. Visualizing DCGAN Samples\n",
    "\n",
    "\n",
    "\n",
    " We use our `DCGenerator` and display a 4√ó4 grid of generated images :contentReference[oaicite:4]{index=4}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate generator\n",
    "G = DCGenerator(noise_size=100, conv_dim=64).cuda()\n",
    "fixed_noise = to_var(torch.randn(16, 100, 1, 1))\n",
    "\n",
    "# Generate samples\n",
    "with torch.no_grad():\n",
    "    fake_images = G(fixed_noise)\n",
    "\n",
    "# Make grid and plot\n",
    "grid = make_grid(fake_images, nrow=4, normalize=True, value_range=(-1,1))\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.imshow(grid.permute(1,2,0))\n",
    "plt.title(\"DCGAN Fake Samples\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 5. DCGAN Training Loss Curves\n",
    "\n",
    "\n",
    "\n",
    " Load your logged losses (saved as NumPy arrays during training)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace with your actual log paths\n",
    "g_losses = np.load('logs/dcgan_g_losses.npy')\n",
    "d_losses = np.load('logs/dcgan_d_losses.npy')\n",
    "iterations = np.arange(len(g_losses)) * 100  # e.g. logged every 100 iters\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(iterations, g_losses, label='Generator Loss')\n",
    "plt.plot(iterations, d_losses, label='Discriminator Loss')\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"DCGAN Training Curves\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 6. CycleGAN Sample Comparisons\n",
    "\n",
    "\n",
    "\n",
    " Display saved sample images from ùëã‚Üíùëå and ùëå‚Üíùëã at iteration 400."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image(path, title):\n",
    "    img = Image.open(path).convert('RGB')\n",
    "    plt.figure(figsize=(4,4))\n",
    "    plt.imshow(img)\n",
    "    plt.title(title)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "show_image('output/cyclegan/sample-000400-X-Y.png', 'CycleGAN X‚ÜíY @400')\n",
    "show_image('output/cyclegan/sample-000400-Y-X.png', 'CycleGAN Y‚ÜíX @400')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 7. Cycle Consistency Loss\n",
    "\n",
    "\n",
    "\n",
    " The cycle loss enforces \\(G(F(y)) \\approx y\\) and \\(F(G(x)) \\approx x\\), typically using L1:\n",
    "\n",
    "\n",
    "\n",
    " \\[\n",
    "\n",
    "   \\mathcal{L}_{cycle}\n",
    "\n",
    "   = \\mathbb{E}_{x\\sim X}\\lVert F(G(x)) - x\\rVert_1\n",
    "\n",
    "   + \\mathbb{E}_{y\\sim Y}\\lVert G(F(y)) - y\\rVert_1.\n",
    "\n",
    " \\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 8. Embedding TensorBoard in‚ÄëNotebook\n",
    "\n",
    "\n",
    "\n",
    " Launch TensorBoard directly in this notebook :contentReference[oaicite:5]{index=5}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In a Jupyter cell, uncomment to launch:\n",
    "# %load_ext tensorboard\n",
    "# %tensorboard --logdir=output/vanilla  # or your CycleGAN logs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
